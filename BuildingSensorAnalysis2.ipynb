{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql import Row\nimport ast\nimport json\nimport pandas as pd\nimport numpy as np\nimport requests\nimport pickle\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport os\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\nfrom pyspark.sql import HiveContext\nhiveContext=HiveContext(sc)\nimport sys\nfrom pyspark.sql.functions import lit\nfrom numpy import *\n#%matplotlib inline\n#dbutils.fs.ls('/mnt/assignment1/csvParquetFiles/')"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#df=sqlContext.table(\"summary\")\n#df.columns\ndf.repartition(1).write.format('com.databricks.spark.csv').option(\"header\", \"true\").save('/mnt/assignment1/allSummary3.csv')"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["def loadUpdatedTags():\n  url=\"https://s3-us-west-2.amazonaws.com/data-analysis-sachin/tags.json\"\n  tagsResponse=requests.get(url)\n  import json\n  j=json.loads(tagsResponse.text)\n  dfs=[]\n  keys=j.keys()\n  for key in keys:\n    for d in j[key]['sensorpoints']:\n      d['template']=j[key]['template']\n      d['name']=j[key]['name']\n      d['sensor_id']=str(key)\n      df=pd.DataFrame.from_dict(d)\n      dfs.append(df)\n  print len(keys)\n  print len(dfs)\n  df=pd.concat(dfs)\n  print df.columns\n  print len(df)\n  return df\n\ntagsdfPd=loadUpdatedTags()\n#tagsDF=sqlContext.createDataFrame(tagsdfPd)\n#spDF.write.parquet(\"/mnt/assignment1/csvParquetFiles/tagsDF\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#df=sqlContext.read.parquet(\"/mnt/assignment1/csvParquetFiles_new/processed_\"+str(633))\n#print df.count()\n#df2=df.select('sensor_id').distinct()\n#df2.collect()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#read data from s3\ndfs={}\nfor i in xrange(700,822):\n  try:\n    path=\"/mnt/assignment1/csvParquetFiles_new/processed_\"+str(i)\n    dfs[i]=hiveContext.read.parquet(path)\n    print i\n    #dfs[i].select('sensor_value').distinct().collect()\n  except:\n    continue"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def getInfo(x,type):\n  try:\n    if(type=='name'):\n      return str(tagsdfPd[tagsdfPd['sensor_id']==x]['name'].tolist()[0])\n    elif(type=='template'):\n      return str(tagsdfPd[tagsdfPd['sensor_id']==x]['template'].tolist()[0])\n  except:\n    print x\n    return \"no room found\"\n\n#print getName('876d4cd6-7808-11e3-9b2e-00163e005319')\n  "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df=dfs[700]\n#counts=[dfs[600].count()]\nfor i in xrange(700,822):\n  #counts.append(dfs[i].count())\n  df=df.unionAll(dfs[i])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#print sum(counts)\n#print df.count()\n#print dfs[600].count()\n#print dfs[601].count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#df2=df.rdd\n#df2=hiveContext.createDataFrame(df.rdd, samplingRatio=0.2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#df=hiveContext.sql(\"select * from allDataBuildingSensorsNew\")\n#df2=df.withColumn('sensor_value',getName(df.sensor_id))\ndf2=df.map(lambda row: Row(**dict(row.asDict(), name=getInfo(row.sensor_id,'name'),template=getInfo(row.sensor_id,'template'))))\n#df.foreach(getName)\ndf2=hiveContext.createDataFrame(df2,samplingRatio=0.00001)\nprint df2.count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["df.write.saveAsTable('data700_822')\n#df2.select(['room']).distinct().collect()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#union s3 data to one df\ndf=dfs[0]\nfor i in xrange(1,300):\n  try:\n    df.unionAll(dfs[i])\n    print i\n  except:\n    continue"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["rooms=list(df.select(['room']).distinct().collect())\nrooms=[s.room for s in rooms]\nprint rooms\n#rooms=[u'Rm-4256', u'RM-1145', u'Rm-3252', u'Rm-2148', u'RM-1150', u'Rm-4152', u'CRAC-4', u'RM-1208', u'Rm-3154', u'Rm-3203', u'Rm-2150', u'Rm-2207', u'RM-1212', u'RM-B260', u'AH1-1', u'RM-1106', u'Rm-3213', u'Rm-3215', u'Rm-2214', u'Rm-2219', u'RM-1229', u'Rm-2109', u'Rm-3221', u'Rm-3223', u'CHWP1', u'CHWP3-VFD', u'RM-B215', u'Rm-3111', u'Rm-2221', u'Rm-3114', u'RM-1124', u'RM-1125', u'not_found', u'RM-1126', u'RM-B222', u'Rm-4127', u'Rm-3126', u'Rm-2236', u'Rm-2238', u'RM-B230', u'RM-1139', u'RF2', u'Rm-4254']\n#df.count() #=8725890"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["DF100={}\nfor i in xrange(251,300):\n  try:\n    dfRead=dfs[i].drop('sensor_value')\n    dfRead=dfRead.dropna()\n    roomsList=list(dfRead.select(['room']).distinct().collect())\n    roomsList=[s.room for s in roomsList]\n    for room in roomsList:\n      dfRoom=dfRead.where(dfRead['room']==room)\n      if(DF100.has_key(room)):\n        DF100[room].unionAll(dfRoom)\n      else:\n        DF100[room]=dfRoom\n  except:\n    continue"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["keys=DF100.keys()\nfor key in keys:\n  try:\n    dfRead=DF100[key]\n    dfRead=dfRead.map(lambda row: Row(**dict(row.asDict(), timeseries=datetime.strptime(row.timeseries,\"%Y-%m-%dT%H:%M:%S+00:00\"))))\n    dfRead=dfRead.map(lambda row: Row(**dict(row.asDict(), date=datetime.date(row.timeseries))))\n    dfRead=dfRead.map(lambda row: Row(**dict(row.asDict(), year=datetime.date(row.timeseries).year)))\n    dfRead=dfRead.map(lambda row: Row(**dict(row.asDict(), sensor_value=getSensorValue(row.sensor_id))))\n    dfRead=sqlContext.createDataFrame(dfRead)\n    dfRead.write.parquet(\"/mnt/assignment1/csvParquetFiles/room300_\"+key)\n  except:\n    print key"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["def load(room,year):\n  path={}\n  path[1]=\"/mnt/assignment1/csvParquetFiles/room100_\"+room\n  path[2]=\"/mnt/assignment1/csvParquetFiles/room200_\"+room\n  path[3]=\"/mnt/assignment1/csvParquetFiles/room300_\"+room\n  path[4]=\"/mnt/assignment1/csvParquetFiles/room250_\"+room\n  dfs=[]\n  for i in xrange(5):\n    try:\n      df=sqlContext.read.parquet(path[i])\n      df=df.filter(df['year']==year)\n      dfs.append(df)\n    except:\n      print i\n  if(len(dfs)>0):\n    df=dfs[0]\n    for i in xrange(1,len(dfs)):\n      df.unionAll(dfs[i])\n    return df\n  else:\n    print \"no data for the room\""],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["def dataPlot(dfArgument,timeLowerBound, timUpperBound):\n  tempDF=dfArgument[dfArgument.timeseries.between(timeLowerBound, timUpperBound)]\n  #if(Rooms!='All'):\n  #  tempDF=tempDF[tempDF['room']==Rooms]\n  display(tempDF)\n  return tempDF"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#load room, year\ndf=load('HW-SYS','2015')\nid=df.select('sensor_id').distinct().collect()\nprint id\n#print \"tag information for the room is \",tagsDict[id[0].sensor_id]\nprint df.select('sensor_value').distinct().collect()\nprint df.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["dfHW_SYS=dataPlot(df,'2015-03-01 08:00:00', '2015-03-08 17:30:00')"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df4126=dataPlot(df,'2015-03-01 08:00:00', '2015-03-08 17:30:00')"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# %load PieceWise.py\n#Encoder class for spark dataframes\nimport pandas as pd\nfrom numpy import *\nclass encoder:\n    \"\"\"\n    The encoder/decoder class is the base class for all encoder/decoder pairs.\n    Subclasses encode different types of encoding.\n    EncoderLearner is a factory class for fitting encoders to data\n    \"\"\"\n    def __init__(self,raw):\n        \"\"\"\n        given a spark DataFrame or Series (raw), find the best model of a given type\n        \"\"\"\n    \n    def compress(self):\n        \"\"\"\n        given a raw sequence and a model, return a compressed representation.\n        \"\"\"\n        self.compressed=None\n        return self.compressed\n    \n    def recon(self,compressed):\n        \"\"\"\n        Recreate the original DataFrame or Series, possibly with errors.\n        \"\"\"\n        Recon=None\n        return Recon\n    \n    def get_size(self):\n        return len(self.compressed)\n    \n    def compute_error(self,S,compressed=None):\n        if type(compressed)==type(None):\n            compressed=self.compressed\n        R=self.recon(compressed=compressed,index=S.index)\n        V=R-S\n        V.dropna()\n        return sqrt(sum([v*v for v in V.values]))/len(V)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["class piecewise_constant(encoder):\n    \"\"\" \n    Represent the signal using a sequence of piecewise constant functions \n    \"\"\"\n    def __init__(self,raw):\n      S=raw\n      if type(S) != pd.Series:\n        raise 'encode expects pandas Series as input'\n      self.index=S.index\n      self.Sol=self.fit(S)\n    \n    # fit uses dynamic programming to find the best piecewise constant solution\n    # max_gap is the maximal extent of a single step.\n    # Reason for max_gap is that even if the error is small we want to correct\n    # it with some minimal frequence. \n    # Not quite a snapshot because the value will not necessarily change after \n    # max_gap is reached.\n    def fit(self,S,max_gap=96):\n        S[isnan(S)]=0\n        _range=np.max(S)-np.min(S)\n        # _range is a constant that is added to the error at each stop point\n        # Larger values will cause fewer switches.\n        print 'range=',_range\n        #Dynamic programming\n        Sol=[[]]*len(S)  # an array that holds the best partition ending at each point of the sequence.\n                # Each element contains a best current value, a pointer to the last change in best \n                # solution so far and the total error of best solution so far.\n        for i in range(len(S)):\n            if i==0:\n                Sol[i]={'prev':None, 'value':S[0], 'error':0.0, 'switch_no':0}\n            # Sol is indexed by the location in the sequence S\n            # prev: the index of the last switch point\n            # value: current prediction value\n            # error: cumulative error to this point\n            # switch_no: number of switches so far.\n            else:\n                err0 = Sol[i-1]['error']+(Sol[i-1]['value']-S[i])**2\n                best=None\n                best_err=1e20\n                best_val=S[i]\n                for j in xrange(np.max([0,i-max_gap]),i):\n                    _mean=np.mean(S[j:i])\n                    _std=np.std(S[j:i])\n                    err=_std*(i-j)+Sol[j]['error']+_range\n                    if err<best_err:\n                        best=j\n                        best_val=_mean\n                        best_err=err\n                Sol[i]={'prev':best, 'value':best_val, 'error':best_err,\\\n                        'switch_no': Sol[best]['switch_no']+1}\n            #print '\\r',i,Sol[i],\n        return Sol\n    \n    def compress(self,S):\n        Switch_points=[]\n        i=len(self.Sol)-1                # start from the end \n        while i>0:\n            prev=self.Sol[i]['prev']\n            value=self.Sol[i]['value']\n            if self.Sol[prev]['value'] != value:\n                Switch_points.append({'time':S.index[prev],'value':value})\n            i=prev\n        self.compressed=Switch_points\n        return Switch_points\n\n    def recon(self,compressed=None, index=None):\n        #print '\\nindex=',index==None,'\\n'\n        #print '\\ncompressed=',compressed==None,'\\n'\n        if(type(index)==type(None)):\n            index=self.index\n        Recon=pd.Series(index=index)\n        \n        if(type(compressed)==type(None)):\n            compressed=self.compressed\n        for e in compressed:\n            time=e['time']\n            value=e['value']\n            Recon[time]=value\n            \n        return Recon.fillna(method='ffill')"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["class piecewise_linear(encoder):\n    \"\"\" \n    Represent the signal using a sequence of piecewise linear functions \n    \"\"\"\n    def __init__(self,raw):\n      S=raw\n      if type(raw) != pd.Series:\n        raise 'encode expects pandas Series as input'\n      self.index=raw.index\n      self.Sol=self.fit(raw)\n    \n    # fit uses dynamic programming to find the best piecewise linear solution\n    # max_gap is the maximal extent of a single step.\n    # Reason for max_gap is that even if the error is small we want to correct\n    # it with some minimal frequence. \n    # Not quite a snapshot because the value will not necessarily change after \n    # max_gap is reached.\n    def fit(self,S,max_gap=96):\n        S[isnan(S)]=0\n        _range=np.max(S)-np.min(S)\n        # _range is a constant that is added to the error at each stop point\n        # Larger values will cause fewer switches.\n        print 'range=',_range\n        #Dynamic programming\n        Sol=[[]]*len(S)  # an array that holds the best partition ending at each point of the sequence.\n                # Each element contains a best current value, a pointer to the last change in best \n                # solution so far and the total error of best solution so far.\n        for i in range(len(S)):\n            if i==0:\n                Sol[i]={'prev':None, 'value':S[0], 'error':0.0, 'switch_no':0, 'slope':0}\n            # Sol is indexed by the location in the sequence S\n            # prev: the index of the last switch point\n            # value: current prediction value\n            # error: cumulative error to this point\n            # switch_no: number of switches so far.\n            #slope: slope of th linear line at this point\n            else:\n                err0 = Sol[i-1]['error']+(Sol[i-1]['value']-S[i])**2\n                best=None\n                best_err=1e20\n                best_val=S[i]\n                best_slope=1e20\n                for j in xrange(np.max([0,i-max_gap]),i):\n                    #_mean=np.mean(S[j:i])\n                    _slope=(S[i]-S[j])*1.0/(i-j)\n                    #_std=np.std(S[j:i]\n                    _val=0\n                    _err=0\n                    for k in xrange(j,i):\n                      _val=Sol[j]['value']+_slope*(k-j)\n                      _err+=(Sol[k]['value']-_val)**2\n                    err=_err*1.0/(i-j)+Sol[j]['error']+_range\n                    _val=Sol[j]['value']+_slope*(i-j)\n                    if err<best_err:\n                        best=j\n                        best_val=_val\n                        best_err=err\n                        best_slope=_slope\n                Sol[i]={'prev':best, 'value':best_val, 'error':best_err,\\\n                        'switch_no': Sol[best]['switch_no']+1, 'slope':best_slope}\n            #print '\\r',i,Sol[i],\n        return Sol\n    \n    def compress(self,S):\n        Switch_points=[]\n        i=len(self.Sol)-1                # start from the end \n        while i>0:\n            prev=self.Sol[i]['prev']\n            slope=self.Sol[i]['slope']\n            value=self.Sol[i]['value']\n            if self.Sol[prev]['slope'] != slope:\n                Switch_points.append({'time':S.index[prev],'value':value})\n            i=prev\n        self.compressed=Switch_points\n        return Switch_points\n\n    def recon(self,compressed=None, index=None):\n        #print '\\nindex=',index==None,'\\n'\n        #print '\\ncompressed=',compressed==None,'\\n'\n        if(type(index)==type(None)):\n            index=self.index\n        Recon=pd.Series(index=index)\n        \n        if(type(compressed)==type(None)):\n            compressed=self.compressed\n        for e in compressed:\n            time=e['time']\n            value=e['value']\n            Recon[time]=value\n            \n        return Recon.fillna(method='ffill')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["def model(A,method):\n  #A.count() #=2093\n  #A.columns  \n  sys.stdout.flush()\n  #try:\n  pd_df=pd.DataFrame(A.select('timeseries','values').limit(5000).collect(),columns=['timeseries','values'])\n  pd_df.plot(kind='line')\n  S=pd_df['values']\n  _std=np.std(S)\n  print \"Std dev is \",_std\n  if(method=='piecewise_constant'):\n    encoder=piecewise_constant(S)\n  elif(method=='piecewise_linear'):\n    encoder=piecewise_linear(S)\n  C=encoder.compress(S)\n  compressed_df=pd.DataFrame(C)\n  print 'size=',encoder.get_size(),\n  error=encoder.compute_error(S,compressed=C)\n  print 'error=',error, 'error/_std=',error/_std\n  print C\n  return compressed_df\n  #except:\n  #  print \"Error!\""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#data to be analyzed - dfHW_SYS for time range '2015-03-01 08:00:00', '2015-03-08 17:30:00'\n\npd_df=pd.DataFrame(dfHW_SYS.select('timeseries','values').limit(5000).collect(),columns=['timeseries','values'])\nfig = plt.figure(figsize=(15, 3))\npd_df.plot(kind='line',ax=fig.gca())\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["compressed_df=model(dfHW_SYS,'piecewise_constant') \ncompressed_df=compressed_df.sort(['time'], ascending=[1])\nprint compressed_df"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#piecewise constant analysis for dfHW_SYS for time range '2015-03-01 08:00:00', '2015-03-08 17:30:00'\nfig = plt.figure(figsize=(15, 3))\ncompressed_df.plot(x='time',y='value',kind='line',ax=fig.gca())\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["compressed_df_linear=model(dfHW_SYS,'piecewise_linear') \ncompressed_df_linear=compressed_df_linear.sort(['time'], ascending=[1])\nprint compressed_df_linear"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["#piecewise linear for dfHW_SYS for time range '2015-03-01 08:00:00', '2015-03-08 17:30:00'\nfig_linear = plt.figure(figsize=(15, 3))\ncompressed_df_linear.plot(x='time',y='value',kind='line',ax=fig_linear.gca())\nprint display(fig_linear)\n\n#since this data is mostly step shifted, its better modeled using piecewise constant as against piecewise linear"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#Old code below"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["A=dfHW_SYS\nA.count()\nA=A.dropna(subset = ['room'])\nprint 'starting Piecewise, rooms=',rooms\nsys.stdout.flush()\ncols = list(A.columns)\ncols.remove('room')\n\nfor room in rooms:\n  try:\n    #room=rooms[0]\n    print '-'*20,room,'-'*20\n    sys.stdout.flush()\n    DF = A[A['room']==room]\n    DF = DF.drop('room')  # remove the entry \"Room\"\n    name='values'\n    pd_df=pd.DataFrame(DF.limit(1000).collect(),columns=['TimeStamp','date','sensor_id','sensor_value','values'])\n    S=pd_df[name]\n    #S=[s.values for s in S][0:200]\n    #Spd=pd.Series(S,name='values') \n    _std=np.std(S)\n    print 'room=',room,'signal=',name, 'std=',_std,\n    encoder=piecewise_constant(S)\n    C=encoder.compress()\n    print 'size=',encoder.get_size(),\n    error=encoder.compute_error(S,compressed=C)\n    print 'error=',error, 'error/_std=',error/_std\n    print C\n  except:\n    print room    "],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["display(df5)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["df.select('sensor_name').distinct().collect()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["display(name_dfs[names2[1]]) #MAIN HW SUPPLY TEMP"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["display(name_dfs[names2[2]]) #Zone Temperature"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["display(name_dfs[names2[3]]) #Something"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["display(name_dfs[names2[4]]) #Warm Cool Adjust"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["display(name_dfs[names2[5]]) #Al 1 Actual"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["display(name_dfs[names2[7]]) #Damper Position"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["display(name_dfs[names2[8]]) #Actual Cooling SetPoint"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["name_dfs={}\nfor name in names2:\n  name_dfs[name]=dataPlot(df,'2014-03-01 08:00:00', '2014-03-08 17:30:00','All',name)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["display(name_dfs[names2[0]]) #Actual heating setpoint"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["display(name_dfs[names2[1]]) #MAIN HW SUPPLY TEMP"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["display(name_dfs[names2[2]]) #zone temp"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["display(name_dfs[names2[3]]) #something"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["display(name_dfs[names2[6]]) #Supply vel press"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["display(name_dfs[names2[7]]) #damper "],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["display(name_dfs[names2[8]]) #Actual cooling setpoint"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["room_dfs={}\nfor room in rooms:\n  room_dfs[room]=dataPlot(df,'2014-01-01 08:00:00', '2014-01-30 17:30:00',room,'All')"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["display(room_dfs[rooms[0]])"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["display(room_dfs[rooms[1]])"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["display(room_dfs[rooms[2]])"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["display(room_dfs[rooms[3]])"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["display(room_dfs[rooms[4]])"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["display(room_dfs[rooms[5]])"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["display(room_dfs[rooms[6]])"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["mayWeek={}\nfor room in rooms:\n  mayWeek[room]=dataPlot(df,'2014-05-06 08:00:00', '2014-05-13 17:30:00',room,'All')"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["display(mayWeek[rooms[0]])"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["display(mayWeek[rooms[1]])"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["display(mayWeek[rooms[3]])"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["display(mayWeek[rooms[5]])"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["display(mayWeek[rooms[6]])"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["#Old code below"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["#Building sensor:room dictionary\ntags=sc.textFile(\"/mnt/assignment1/tags.json\")\ntags.cache()\n#All sc.textfile reads are single line\ntagsDict=ast.literal_eval(tags.take(1)[0])\nsensorRoomDict={}\nfor k in tagsDict.keys():\n  ctxts=tagsDict[k]['contexts']\n  for c in ctxts:\n    if(c['keyword']=='room'):\n        sensorRoomDict[str(k)]=str(c['tag'])\n        \nprint sensorRoomDict['876d4cd6-7808-11e3-9b2e-00163e005319']"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["def jsonParser(line):\n  dictRow={}\n  row=line.split(\"}]}}\")\n  for r in row:\n    splits=r.split('\"')\n    if(len(splits)>=2):\n      dictRow[splits[1]]=[splits[3],\n    \n  return dictRow"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["#Reading raw_data jsons - simple small file - rawdata_047.json #Using json.loads <-- doesn't ork for most given jsons\nrawdata_047 = sc.textFile(\"/mnt/assignment1/rawdata_047.json\")\nrawdata_047.cache()\nrd047=rawdata_047.take(1)\nrd047df=json.loads(str(rd047[0]))\nrd047df['33c56e1e-8917-11e2-8a68-00163e005319']['PresentValue']['timeseries']"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["#from pyspark.sql import SQLContext\n#sqlContext = SQLContext\n#df_021=sqlContext.read.json(\"/mnt/assignment1/rawDataJsons/DF_085.json\")\n#df_044=sc.textFile(\"/mnt/assignment1/rawDataJsons/DF_085.json\", minPartitions = None, use_unicode = True)\n#df_021=sc.textFile(\"/mnt/assignment1/rawDataJsons/DF_021.json\", minPartitions = 100)\n#df_021_1=sc.textFile(\"/mnt/assignment1/rawDataJsons/DF_021_split1.json\")\ndf_021_2=sqlContext.read.json(\"/mnt/assignment1/rawDataJsons/DF_021_split2.json\")"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["df_021_2.head()"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["#t=df_044.collect()\n#df=pd.DataFrame(str(t[0]))\n#x=ast.literal_eval(t[0])\n#df44=pd.DataFrame(x)\n#df44\n#t[0][-100:-1]\n#t21_1=df_021_1.collect()\nt21_1"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["rawdata_065=sc.textFile(\"/mnt/assignment1/rawdata/rawdata_065.json\")\nrawdata_066 = sc.textFile(\"/mnt/assignment1/rawdata/rawdata_066.json\")"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["tagsDict['e34eb68e-5882-4860-bb40-7832bb7e8a70']"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["l=rawdata_066.collect()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["l2[0].split('\"')"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["splits[0].split('\"')"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["rd066=rawdata_066.map(lambda r:jsonParser(r))"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["rd066DF=rd066.toDF()\nrd066DF.columns"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["rd010=sqlContext.read.json(\"/mnt/assignment1/rawdata/rawdata_010.json\")\n#l009=rd010.repartition(100).collect()"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["rd065=rawdata_065.map(lambda r:jsonParser(r))"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["rd065DF=rd065.toDF()\nrd065DF.columns #Non existing column in tag"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["rd065DF.collect()"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["tagsDict.keys()"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":91}],"metadata":{"name":"BuildingSensorAnalysis2","notebookId":14769},"nbformat":4,"nbformat_minor":0}
