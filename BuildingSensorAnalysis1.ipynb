{"cells":[{"cell_type":"code","source":["from pyspark.sql import Row\nfrom pyspark.sql.types import *\nimport ast\nimport json\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\nfrom pyspark.sql import HiveContext\nhiveContext=HiveContext(sc)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#dbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#The below code needs to be run on an ec2 machine after downloading the cv files. \n#The code generates processed json files and pushes it to s3\n\nimport pandas as pd\nimport json\nimport ast\nimport requests\nimport pickle\nimport os\n\ndef loadUpdatedTags():\n\turl=\"https://s3-us-west-2.amazonaws.com/data-analysis-sachin/tags.json\"\n\ttagsResponse=requests.get(url)\n\tj=json.loads(tagsResponse.text)\n\tdfs=[]\n\tkeys=j.keys()\n\tddict={}\n\tfor key in keys:\n\t\ttemplate=[]\n\t\tfor d in j[key]['contexts']:\n\t\t\tif(d['keyword']=='room'):\n\t\t\t\troom=d['tag']\n\t\tfor d in j[key]['sensorpoints']:\n\t\t\ttemplate.append(d['description'])\n\t\tddict[str(key)]={'sensor_id':str(key),'room':room,'template':template}\n\treturn ddict\n\nsensorRoomDict=loadUpdatedTags()\n#print sensorRoomDict['876d4cd6-7808-11e3-9b2e-00163e005319']\ndef csvParser(path):\n    #print \"starting \",path\n    dfs={}\n    with open(path) as f:\n        l=f.readlines()\n    print \"lines=\",len(l)\n    i=0\n    while(i<len(l)):\n        try:\n            #print path[:-4]\n            df=pd.DataFrame(columns=[\"sensor_id\",\"timeseries\",\"values\",\"room\",\"template\"])\n            #df[\"sensor_value\"]=l[i].strip()\n            df[\"timeseries\"]=l[i+1].split(',')\n            df[\"values\"]=l[i+2].split(',')\n            df[\"sensor_id\"]=path[:-4]\n            try:\n                df[\"room\"]=sensorRoomDict[path[:-4]]['room']\n            except:\n                df[\"room\"]=\"not_found\"\n            try:\n            \tdf['template']=sensorRoomDict[path[:-4]]['template'][i/3]\n            except:\n                df[\"template\"]=\"not_found\"\n            dfs[path[:-4]]=df\n            i+=3\n        except:\n            print \"parser exception \",i/3\n            i+=3\n    return dfs\n\n#write loop to read csv and push dfs to s3\ndirectory=os.getcwd()\npaths=os.listdir(directory)\nj=0\nfor i in xrange(len(paths)):\n    if(i<0):\n        continue\n    else:\n        try:\n            #print \"starting \",i,paths[i]\n            dfDict=csvParser(paths[i])\n            for k,v in dfDict.iteritems():\n                f=\"DF_\"+str(k)+\".json\"\n                print f\n                v.to_json(f,orient=\"split\")\n                url=\"s3cmd put --acl-public --guess-mime-type \"+f+\" s3://data-analysis-sachin/csv2jsons_new/\"+f\n                os.system(url)\n                cmd=\"rm \"+f\n                os.system(cmd)\n                j+=1\n        except:\n            print \"exception in \",i,paths[i]\n\n\n\ndef jsonParser(line):\n    dictRow={}\n    row=line.split(\"}}\")\n    i=0\n    dfs=[]\n    while i<len(row):\n        try:\n            r,t=row[i].split('timeseries\":')\n            splits=r.split('\"')\n            timeValues=json.loads(t)\n            timeValuesDict={}\n            for tv in timeValues:\n                timeValuesDict[tv.keys()[0]]=tv.values()[0]\n            tv_df=pd.DataFrame.from_dict(timeValuesDict,orient='index')\n            if(len(splits)>=2):\n                room=\"\"\n                if splits[1] in sensorRoomDict.keys():\n                    room=sensorRoomDict[splits[1]]\n                tv_df.columns=[\"values\"]\n                print splits[3]\n                tv_df['sensor_value']=splits[3]\n                tv_df['room']=sensorRoomDict[splits[1]]\n                tv_df['sensor_id']=splits[1]\n                dfs.append(tv_df)\n        except:\n            print \"inside file exception - row\"+str(i)\n        i=i+1\n    if(len(dfs)>0):\n        combined_df=pd.concat(dfs)\n        return combined_df\n    else:\n         return \"no dataframe\"  "],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#Code to read jsons from s3 and save them as parquet files\ndef readData(m,n):\n  pd_df_dict={}\n  for i in xrange(m,n):\n      try:\n        if(i%100==0):\n          print i\n        pd_df_dict[i]=pd.read_json(\"s3://data-analysis-sachin/csv2jsons_new/DF_\"+str(i)+\".json\",orient='split')\n      except:\n        print \"exception\", i\n  return pd_df_dict\n\nk=0\nfor i in xrange(0,16440,20):\n  j=i+20\n  pd_df_dict=readData(i,j)\n  pd_all_concat=pd.concat(pd_df_dict.values())\n  sp_df_all=sqlContext.createDataFrame(pd_all_concat)\n  path=\"/mnt/assignment1/csvParquetFiles_new/processed_\"+str(k)\n  sp_df_all.write.parquet(path)\n  k+=1"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Old/outdated analysis/code below"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["timeCorrected_DF=timeCorrected_DF.withColumn('TimeStamp', from_unixtime('TimeStamp', format='yyyy-MM-dd HH:mm:ss'))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["timeCorrected_DF.saveAsTable(\"batch1_buildingSensorAnalysis_timeCorrected\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["timeCorrected_DF=sqlContext.table('batch1_buildingSensorAnalysis_timeCorrected')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Time Slicing #\ntimeSliced_DF=timeCorrected_DF[timeCorrected_DF['TimeStamp']<='2013-12-27 17:30:00']\ntimeSliced_DF=timeSliced_DF[timeSliced_DF['TimeStamp']>='2013-12-27 09:00:00']"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["timeSliced_DF.count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["timeSliced_DF.columns"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["RoomGrouped_DF=timeSliced_DF.select('Room','Actual_Cooling_Setpoint',\n 'Actual_Heating_Setpoint','Temperature','Zone_Temperature').groupBy('Room')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["RoomGrouped_DF.avg().show() #RoomWise average for the sensors on the day 27th Dec 8am-5.30pm"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["### Visualizations ###\ndisplay(timeSliced_DF)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(timeSliced_DF)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(timeSliced_DF)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(timeSliced_DF)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(timeSliced_DF)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["def dataPlot(dfArgument,timeLowerBound, timUpperBound, Rooms):\n  tempDF=dfArgument[dfArgument.TimeStamp.between(timeLowerBound, timUpperBound)]\n  if(Rooms!='All'):\n    tempDF=tempDF[tempDF['room']==Rooms]    \n  display(tempDF)\n  return tempDF"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["timeSlicedRoomFilteredDF=dataPlot(timeCorrected_DF,'2014-04-23 08:00:00', '2014-04-30 17:30:00',4126)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(timeSlicedRoomFilteredDF.withColumn('Actual_Supply_Flow',timeSlicedRoomFilteredDF.Actual_Supply_Flow/20.+50))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["#Reading Parquet Files"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.sql import HiveContext\nhiveContext=HiveContext(sc)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["dfs={}\nfor i in xrange(19):\n  path=\"/mnt/assignment1/parquetFiles/processed_\"+str(i)\n  dfs[i]=hiveContext.read.parquet(path)\n  print dfs[i].select('sensor_value').distinct().collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["df=dfs[0]\nfor i in xrange(1,19):\n  df.unionAll(dfs[i])"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from datetime import datetime\ndf2=df.map(lambda row: Row(**dict(row.asDict(), TimeStamp=datetime.strptime(row.TimeStamp,\"%d-%m-%y %H:%M\"))))\ndf3=sqlContext.createDataFrame(df2)\ndf3.describe"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["df3.saveAsTable('rd0_19_del_others')"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["df=df3\ndf.count()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.sql import functions as F\nprint df.agg(F.min(df.TimeStamp)).collect()\ndf.agg(F.max(df.TimeStamp)).collect()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["df.groupBy('room','sensor_id').count().collect()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["df.select('room').distinct().collect()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["dfb103A=df.where(df.room=='RM-B103A')"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["df2=df.map(lambda row: Row(**dict(row.asDict(), date=datetime.date(row.TimeStamp))))\ndf=sqlContext.createDataFrame(df2)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["dfPlot=dataPlot(df,'2014-01-01 08:00:00', '2014-12-30 17:30:00', 'RM-B103A')"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["display(dfPlot)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["#Getting dfPlot in pandas\ndays=dfPlot.select('date').distinct().collect()\ndays=[d.date for d in days]\nprint dfPlot.columns\npdPlot=pd.DataFrame(dfPlot.collect(),columns=['TimeStamp','date','room','sensor_id','sensor_value','values'])\ntemp=pdPlot['TimeStamp']\nt=temp.map(lambda x:datetime.time(x))\npdPlot['dayTime']=t"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["#Plotting Pandas DF\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nfor day in days[0:5]:\n  pdSubPlot=pdPlot[pdPlot['date']==day]\n  print pdSubPlot.columns\n  pdSubPlot=pdSubPlot.sort(['dayTime'],ascending=True)\n  print pdSubPlot[0:4]\n  plt.plot(pdSubPlot['dayTime'],pdSubPlot['values'])"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["display(fig)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["#Encder series based on pandas DF. To be modified for Spark"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["pdPlot.columns"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["import sys\nA=pdPlot\nprint 'shape before dropping Room==nan =',shape(A)\nA=A.dropna(subset=['room'])\nprint 'shape after dropping Room==nan =',shape(A)\nrooms=sorted(list(set(A.room.values)))\n\nprint 'starting Piecewise, rooms=',rooms\nsys.stdout.flush()\n\ncols = list(A.columns)\ncols.remove('room')\n#cols.remove('Temperature')\n"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["#import numpy as np\nshape(A['values'])"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["room=rooms[0]\nprint '-'*20,room,'-'*20\nsys.stdout.flush()\nDF = A[A['room']==room]\nDF = DF.drop('room', 1)  # remove the entry \"Room\"\nname='values'\nS=DF[name][0:200]\n_std=std(S)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from numpy import *\nprint 'room=',room,'signal=',name, 'std=',_std,\nencoder=piecewise_constant(S)\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["C=encoder.compress()\nprint 'size=',encoder.get_size(),\nerror=encoder.compute_error(S,compressed=C)\nprint 'error=',error, 'error/_std=',error/_std"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["C"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["### Old code below ###"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["pd_df1=pd.read_json(\"s3://data-analysis-sachin/jsonfiles/150days_split\"+str(1)+\".json\")"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["pd_df_concat=pd.concat([pd_df0,pd_df1])"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["pd_df_concat.shape"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["sp_df=sqlContext.createDataFrame(pd_df_concat)\nsp_df.head(5)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["sp_df.saveAsTable(\"test\")"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["sp_df.write.parquet(\"/mnt/assignment1/test.parquet\")"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["dbutils.fs.ls('/mnt/assignment1/test.parquet/')"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["rawdata_009=sc.textFile(\"/mnt/assignment1/rawdata_009.json\")"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["rd009=rawdata_009.map(lambda r:r.split('PresentValue'))"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["rd009.collect()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["import ast\n#rawdata_047 = sc.textFile(\"/mnt/assignment1/rawdata_047.json\")\n#rd047=rawdata_047.take(1)\n#testdf=pd.read_json(rd047[0])\ntestdf[u'2f1dbf56-8917-11e2-b769-00163e005319']"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["rawdata_047."],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["import numpy as np\nimport math\nAllData={}\nfor i in xrange(10):\n  #print a[a.columns[i]][u'Test SensorPoint']\n  tag=tags[c.columns[i]]['contexts']\n  for t in tag:\n    if(t['keyword']=='room'):\n      room=str(t['tag'])\n      roomSensors={}\n  for k in c[c.columns[i]].keys():\n    if(type(c[c.columns[i]][k])==dict):\n      if('timeseries' in c[c.columns[i]][k].keys()):\n        time={}\n        for d in c[c.columns[i]][k]['timeseries']:\n          time[d.keys()[0]]=d.values()[0]\n      if(k in roomSensors.keys()):\n        roomSensors[k].update(time)\n      else:\n        roomSensors[k]=time\n  \n  sensorTimeDF=pd.DataFrame.from_dict(roomSensors,orient='index')\n  #timeDF.columns=['time']\n  if(room in AllData.keys()):\n    oldDf=AllData[room]\n    AllData[room]=pd.concat([oldDF,sensorTimeDF])\n  else:\n    AllData[room]=sensorTimeDF\n  \nprint AllData['RM-1109']\n  #AllData[room]\n  "],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["files={f1:pd.read_json(\"s3://data-analysis-sachin/Copy of rawdata_054.json\"),f2:pd.read_json(\"s3://data-analysis-sachin/Copy of rawdata_055.json\"),f3:pd.read_json(\"s3://data-analysis-sachin/Copy of rawdata_056.json\"),f4:pd.read_json(\"s3://data-analysis-sachin/Copy of rawdata_057.json\"),f5:pd.read_json(\"s3://data-analysis-sachin/Copy of rawdata_058.json\")}"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\ntags=pd.read_json(\"s3://data-analysis-sachin/tags.json\")\nimport math\nfileDataDict={}\nfileDataDF={}\nfor fileNum in xrange(54,59):\n  f=pd.read_json(\"s3://data-analysis-sachin/Copy of rawdata_0\"+str(fileNum)+\".json\")\n  AllSensorData={}\n  for i in xrange(len(f.columns)):\n    tag=tags[f.columns[i]]['contexts']\n    roomSensor={}\n    for t in tag:\n      if(t['keyword']=='room'):\n        room=str(t['tag'])\n        print room\n    for k in f[f.columns[i]].keys():\n      #df=pd.DataFrame([[0,0,0]],columns=['time',k,'Room'])\n      if(type(f[f.columns[i]][k])==dict):\n        if('timeseries' in f[f.columns[i]][k].keys()):\n          values=[]\n          for d in f[f.columns[i]][k]['timeseries']:\n            values.append([d.keys()[0],d.values()[0],room])\n          \n          df=pd.DataFrame(values,columns=['time',k,'Room'])\n          if(room not in roomSensor.keys()):\n            roomSensor[room]=df\n          else:\n            old_df=roomSensor[room]\n            roomSensor[room]=pd.merge(old_df,df,on=['time','Room'],how='right')\n    AllSensorData[i]=pd.concat(roomSensor.values())\n  fileDataDF[f]=pd.concat(AllSensorData.values())\n  fileDataDict[f]=AllSensorData[i]\n\n\nprint fileDataDF.head()\n"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["AllData['RM-1108'].columns"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["df=pd.DataFrame([[0,0,0]],columns=['time','k','Room'])\ndf = pd.DataFrame([[1, 2], [3, 4]])\nprint df\n#df.append([[5,6]])\ndf2=pd.DataFrame([[5,6]])\nprint df2\ndictodf={}\ndictodf['a']=df\ndictodf['b']=df2\ndf3=pd.concat(dictodf.values())\nprint df3"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["import numpy as np\nimport math\nAllData={}\nfor i in xrange(10):\n  #print a[a.columns[i]][u'Test SensorPoint']\n  tag=tags[c.columns[i]]['contexts']\n  for t in tag:\n    if(t['keyword']=='room'):\n      room=str(t['tag'])\n      if(room in AllData.keys()):\n        roomSensors=AllData[room]\n      else:\n          roomSensors={}\n  for k in c[c.columns[i]].keys():\n    if(type(c[c.columns[i]][k])==dict):\n      if('timeseries' in c[c.columns[i]][k].keys()):\n        time={}\n        for d in c[c.columns[i]][k]['timeseries']:\n          time[d.keys()[0]]=d.values()[0]\n      timeDF=pd.DataFrame.from_dict(time,orient='index')\n      if(k in roomSensors.keys()):\n        roomSensors[k].concat(timeDF)\n      else:\n        roomSensors[k]=timeDF\n  if(room in AllData.keys()):\n    AllData[room].update(roomSensors)\n  else:\n    AllData[room]=roomSensors\n  \nprint AllData\n  #AllData[room]\n\n#for k in c[c.columns[0]].keys():\n#   if(type(c[c.columns[0]][k])==dict):\n#      if('timeseries' in c[c.columns[0]][k].keys()):\n#        print len(c[c.columns[0]][k]['timeseries'])\n#for i in range(0,9):\n  #print len(c[c.columns[0]].to_dict().values()[0]['timeseries'])\n#  print type(a[a.columns[i]]['Test SensorPoint'])\n  #print a[a.columns[i]].to_dict()\n"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["for i in xrange(54,58):\n  print \"s3://data-analysis-sachin/Copy of rawdata_0\"+str(i)+\".json\""],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":73}],"metadata":{"name":"BuildingSensorAnalysis1","notebookId":14624},"nbformat":4,"nbformat_minor":0}
